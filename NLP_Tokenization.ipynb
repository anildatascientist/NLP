{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNoMuF+g0o2OL6xyXsEAbnm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anildatascientist/NLP/blob/main/NLP_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##<b>Tokenization"
      ],
      "metadata": {
        "id": "gARQocOcQENd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization** is the process of breaking down text into smaller units called **tokens**.\n",
        "\n",
        "These **token** can be words or senteneces, thus the terms, **word-tokenization** and **sentence-tokenization**."
      ],
      "metadata": {
        "id": "KiuygGIEPKLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<b>*We will try to achive tokenization with the following 3 ways:*\n",
        "\n",
        "1. Using python methods\n",
        "2. Using regular expression\n",
        "3. Using libraries\n"
      ],
      "metadata": {
        "id": "H4dUuMNIQrsc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> 1. Using python method"
      ],
      "metadata": {
        "id": "iTDfrnDRReuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text1= \"\"\"In Natural Language Processing we want to make computer programs that understand, generate and, more generally speaking, work with human languages.\n",
        "Sounds great!\n",
        "But there‚Äôs a challenge that jumps out: we, humans, communicate with words and sentences; meanwhile, computers only understand numbers.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QI_SUR8-PMD8"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Token\n",
        "word_tokens = text1.split()\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOnMf_1zXBsq",
        "outputId": "495925c0-56b6-405a-ab22-971c3d1f929f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'Natural', 'Language', 'Processing', 'we', 'want', 'to', 'make', 'computer', 'programs', 'that', 'understand,', 'generate', 'and,', 'more', 'generally', 'speaking,', 'work', 'with', 'human', 'languages.', 'Sounds', 'great!', 'But', 'there‚Äôs', 'a', 'challenge', 'that', 'jumps', 'out:', 'we,', 'humans,', 'communicate', 'with', 'words', 'and', 'sentences;', 'meanwhile,', 'computers', 'only', 'understand', 'numbers.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens = text1.split('.')\n",
        "display(sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "hZaUphnOXKYz",
        "outputId": "f3c00570-4c2e-4e40-9d44-4d084790ad4e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['In Natural Language Processing we want to make computer programs that understand, generate and, more generally speaking, work with human languages',\n",
              " ' \\nSounds great! \\nBut there‚Äôs a challenge that jumps out: we, humans, communicate with words and sentences; meanwhile, computers only understand numbers',\n",
              " '\\n']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> 2. Using regular expression"
      ],
      "metadata": {
        "id": "4bzZsU0NX4xB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing regular expression library by the name of \"re\"\n",
        "import re"
      ],
      "metadata": {
        "id": "d3aKLvuoXnaG"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text2= \"\"\"In Natural Language Processing we want to make computer programs that understand, generate and, more generally speaking, work with human languages.\n",
        "Sounds great!\n",
        "But there‚Äôs a challenge that jumps out: we, humans, communicate with words and sentences; meanwhile, computers only understand numbers.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "pIJ1mqJeYdn0"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens_re = re.findall(\"[\\w]+\", text2)\n",
        "print(word_tokens_re)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vet-EOCiYH0y",
        "outputId": "a8b453ae-06b8-4949-9a54-bca249240c78"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'Natural', 'Language', 'Processing', 'we', 'want', 'to', 'make', 'computer', 'programs', 'that', 'understand', 'generate', 'and', 'more', 'generally', 'speaking', 'work', 'with', 'human', 'languages', 'Sounds', 'great', 'But', 'there', 's', 'a', 'challenge', 'that', 'jumps', 'out', 'we', 'humans', 'communicate', 'with', 'words', 'and', 'sentences', 'meanwhile', 'computers', 'only', 'understand', 'numbers']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens_re = re.compile(\"[.!]\").split(text2)\n",
        "sentence_tokens_re"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "702zbQNpYlW4",
        "outputId": "5510ee7c-2867-4d10-8187-78e0f083d01a"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In Natural Language Processing we want to make computer programs that understand, generate and, more generally speaking, work with human languages',\n",
              " ' \\nSounds great',\n",
              " ' \\nBut there‚Äôs a challenge that jumps out: we, humans, communicate with words and sentences; meanwhile, computers only understand numbers',\n",
              " '\\n']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> 3. Using libraries"
      ],
      "metadata": {
        "id": "iB46QzhbaRqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <b> 3.1: NLTK Library"
      ],
      "metadata": {
        "id": "0tI2MosmsCCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing NLTK library\n",
        "!pip install --user -U nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ju5mfVTiaMlP",
        "outputId": "e6c13d2a-a006-448b-ee08-94e1be7d5f27"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Tokenization using NLTK library\n",
        "\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "HCZmgtT4ZxiN"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text3= \"\"\"In Natural Language Processing we want to make computer programs that understand, generate and, more generally speaking, work with human languages.\n",
        "Sounds great!\n",
        "But there‚Äôs a challenge that jumps out: we, humans, communicate with words and sentences; meanwhile, computers only understand numbers.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "MVMpM4Y3qR1u"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens_nltk = word_tokenize(text3)\n",
        "print(word_tokens_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_I66A2E3qWao",
        "outputId": "180a83e7-7775-4efc-b1a3-5c5203c9b644"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'Natural', 'Language', 'Processing', 'we', 'want', 'to', 'make', 'computer', 'programs', 'that', 'understand', ',', 'generate', 'and', ',', 'more', 'generally', 'speaking', ',', 'work', 'with', 'human', 'languages', '.', 'Sounds', 'great', '!', 'But', 'there', '‚Äô', 's', 'a', 'challenge', 'that', 'jumps', 'out', ':', 'we', ',', 'humans', ',', 'communicate', 'with', 'words', 'and', 'sentences', ';', 'meanwhile', ',', 'computers', 'only', 'understand', 'numbers', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization using NLTK library\n",
        "\n",
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "ou_itwkgqfne"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens_nltk = sent_tokenize(text3)\n",
        "display(sentence_tokens_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "0mRvj5a3rixk",
        "outputId": "4342b512-0b33-420d-9f85-073c312b88ac"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['In Natural Language Processing we want to make computer programs that understand, generate and, more generally speaking, work with human languages.',\n",
              " 'Sounds great!',\n",
              " 'But there‚Äôs a challenge that jumps out: we, humans, communicate with words and sentences; meanwhile, computers only understand numbers.']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <b> 3.2 Spacy Library"
      ],
      "metadata": {
        "id": "i2IyhtQdsQfm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing dependencies\n",
        "!pip install spacy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHGwyXefr0Ts",
        "outputId": "eef3eac1-8592-4f1d-b936-f0cfd99b880d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.6.1)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.4)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.1.12)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.7)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.9)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.0)\n",
            "Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.10.2)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.3.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.23.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (23.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.5.0)\n",
            "Requirement already satisfied: pydantic-core==2.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.6.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2023.7.22)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.7.10)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy) (0.1.1)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <b> <i> Word Tokenization using Spacy Library"
      ],
      "metadata": {
        "id": "Yv3yMSKNxbAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the Spacy Library into the Python environment\n",
        "\n",
        "import spacy"
      ],
      "metadata": {
        "id": "FuuGrE34v3zz"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading English Language Model using Spacy Library\n",
        "\n",
        "eng_model = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "_k2fxZwPv9LA"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text4= \"\"\"In Natural Language Processing we want to make computer programs that understand, generate and, more generally speaking, work with human languages.\n",
        "Sounds great!\n",
        "But there‚Äôs a challenge that jumps out: we, humans, communicate with words and sentences; meanwhile, computers only understand numbers.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "9kvMJXV6wmhr"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_tokens_spacy = eng_model(text4)"
      ],
      "metadata": {
        "id": "FlC0JVKIv9Oe"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print([token.text for token in word_tokens_spacy])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkKkWBCSv9TI",
        "outputId": "2d6c2c82-2d85-4e96-9850-2ff9d95802c3"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'Natural', 'Language', 'Processing', 'we', 'want', 'to', 'make', 'computer', 'programs', 'that', 'understand', ',', 'generate', 'and', ',', 'more', 'generally', 'speaking', ',', 'work', 'with', 'human', 'languages', '.', '\\n', 'Sounds', 'great', '!', '\\n', 'But', 'there', '‚Äôs', 'a', 'challenge', 'that', 'jumps', 'out', ':', 'we', ',', 'humans', ',', 'communicate', 'with', 'words', 'and', 'sentences', ';', 'meanwhile', ',', 'computers', 'only', 'understand', 'numbers', '.', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### <b> <i> Sentence Tokenization using Spacy Library"
      ],
      "metadata": {
        "id": "aMiA_SdhxonA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "eng_model = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "s-T25t7Fxqsg"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text4= \"\"\"In Natural Language Processing we want to make computer programs that understand, generate and, more generally speaking, work with human languages.\n",
        "Sounds great!\n",
        "But there‚Äôs a challenge that jumps out: we, humans, communicate with words and sentences; meanwhile, computers only understand numbers.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "yQy6ZVw_x088"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_tokens_spacy = eng_model(text4)"
      ],
      "metadata": {
        "id": "hZLhRV1Xx3uz"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display([sent.text for sent in sentence_tokens_spacy.sents])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "O860h8tAx-Gl",
        "outputId": "fdeae5b9-9f02-4751-b8a7-180e694e2a84"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "['In Natural Language Processing we want to make computer programs that understand, generate and, more generally speaking, work with human languages. \\n',\n",
              " 'Sounds great! \\n',\n",
              " 'But there‚Äôs a challenge that jumps out: we, humans, communicate with words and sentences; meanwhile, computers only understand numbers.\\n']"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### <b> Which Tokenization Should we use?"
      ],
      "metadata": {
        "id": "FHhZtmUv5m1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import TweetTokenizer from nltk.tokenizer module\n",
        "\n",
        "from nltk.tokenize import TweetTokenizer"
      ],
      "metadata": {
        "id": "wiwyELIIr1ig"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text5 = \"\"\"\n",
        "Shivan teaches at Success Analytics. He has also started NLP batch! ‚ò∫Ô∏èü•∞üí™üî•\n",
        "\"\"\"\n",
        "\n",
        "smart_tokenizer = TweetTokenizer(text5)\n",
        "\n",
        "print(smart_tokenizer.tokenize(text5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8hdz5vJ5vPx",
        "outputId": "e6750a3d-1f13-42d7-9db1-fd10fe6431a5"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Shivan', 'teaches', 'at', 'Success', 'Analytics', '.', 'He', 'has', 'also', 'started', 'NLP', 'batch', '!', '‚ò∫', 'Ô∏è', 'ü•∞', 'üí™', 'üî•']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "token_split_method = text5.split()\n",
        "print(token_split_method)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-qylso35vTJ",
        "outputId": "b546fbf0-94b0-4157-e807-7b90557c7b34"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Shivan', 'teaches', 'at', 'Success', 'Analytics.', 'He', 'has', 'also', 'started', 'NLP', 'batch!', '‚ò∫Ô∏èü•∞üí™üî•']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_regular_expression = re.findall(\"[\\w]+\", text5)\n",
        "print(tokens_regular_expression)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8Zlt03d5vWh",
        "outputId": "5e714b9f-0a7a-40e6-b91c-31e8b8b8048c"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Shivan', 'teaches', 'at', 'Success', 'Analytics', 'He', 'has', 'also', 'started', 'NLP', 'batch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_nltk = word_tokenize(text5)\n",
        "print(tokens_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyK0Pfn-7TFp",
        "outputId": "ef5fb15d-1192-455a-c6e5-a4fcb635e8c8"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Shivan', 'teaches', 'at', 'Success', 'Analytics', '.', 'He', 'has', 'also', 'started', 'NLP', 'batch', '!', '‚ò∫Ô∏èü•∞üí™üî•']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens_spacy = eng_model(text5)\n",
        "print([token.text for token in tokens_spacy])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qm2-NI2839Q",
        "outputId": "fd3c2074-7f8c-4742-a775-dd29ffcf7679"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', 'Shivan', 'teaches', 'at', 'Success', 'Analytics', '.', 'He', 'has', 'also', 'started', 'NLP', 'batch', '!', '‚ò∫', 'Ô∏è', 'ü•∞', 'üí™', 'üî•', '\\n']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### <b> As evident from above snippets of code, TweetTokenizer from nltk.tokenize library is best for Tokenization!!!"
      ],
      "metadata": {
        "id": "FstQqF0e9Lwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######################################################################################## THE END #############################################################################"
      ],
      "metadata": {
        "id": "ZLq69qnz9Gcs"
      },
      "execution_count": 108,
      "outputs": []
    }
  ]
}